## spiders 配置方法
```
SPIDERS = {
    "bluefly": {# spider名称对应spider的属性，属性可以根据各自己需求添加，程序运行时会将使用这些属性构造spider类
        "allowed_domain" : [ "bluefly.com", ] # 熟悉scrapy的肯定知道这个属性是用来过滤域名的
        "have_duplicate" : True # 如果同一个spider中项目有重复（这里重复是指product_id重复，也就是说每个项目需要你提供一个product_id来标识），使用这个配置项可以去除重复
        "errback": lambda self, error: "提供自定义errback函数", 
        "parse": lambda self, response: "甚至可以在这里直接定义parse及parse_item函数",
        "parse_item": lambda self, response: "使用lambda函数肯定不是一个好主意，应该在其它文件中定义好函数然后导入到这个模块中使用"
    } 
}
```
## page_xpath 配置方法
现代的web网页中，翻页策略千变万化，框架的功能很难完全支持，目前框架支持4种翻页策略，几乎可以满足90以上网站的使用
### 使用xpath翻页
使用xpath直接将网页中下一页的url匹配出来，这种方式最为常见，一些比较简单的网站会使用这种方式。此类情况一般是分类页响应时html中就已经包含下一页的链接信息，所以实现起来最容易。有些网站xpath可能会经常变来变去，所以这种策略支持同时配置多个xpath,不管变成什么样，总有一款适合你，不用担心会抓取重复，框架已实现了去重功能，demo 如下
```
"bluefly": [ # 你的爬虫的名字，一般可以是网站的名字
        '//*[@id="page-content"]//a[@rel="next"]/@href', # 翻页表达式
        ... # 如果翻页表达式为xpath，则可以定义多个表达式，所得到的结果为所有xpath结果的总和，也就是说xpath匹配到多少下一页链接，就会去重之后生成多少下一页任务请求
    ],
```
### 通过在链接后面加参数实现翻页
demo 如下
```
"jomashop": [ # 你的爬虫的名字，一般可以是网站的名字
        r'(.*?)(p=1)(\d+)(.*)', # 翻页表达式
    ],
```
有必要解释一下这个翻页表达式(正则表达式)：
- 第一组匹配url分页参数前面的部分(不需要修改)
- 第二组匹配分页参数的名字+等号+起始页的页序数(这个要改，比如你的网站翻页是通过增加page参数来实现的，那么这里面就是page=,后面的数字代表你当前是第几页，如果下一页是page=2，那么这个数字填1,如果下一页page=3,那么这个数字填2)
- 第三组匹配当前页数(不需要修改)
- 第四组匹配分页参数后来的值(不需要修改)
比如http://www.nike.com/abc，第二页为http://www.nike.com/abc?pn=1，第三页为http://www.nike.com/abc?pn=2。

### 通过在链接后面加起始项目序号来实现翻页
demo如下
```
"hushpuppies": [
        'start',
    ],
```
配置很简单，start的意思是指标记item开始的那个参数，比如有些网站，第一页没有start，第二页变成了start=30，第三页变成了start=60,以此类推你只要把这个参数名称配置上，就可以了。比如http://www.ecco.com/abc，第二页为http://www.ecco.com/abc?start=30，第三页为http://www.ecco.com/abc?start=60。

### 将页数直接嵌入url中来实现翻页
demo如下
```
'timberland': [
        r'subpath=(/page/)(\d+)(/)',
```
- 其中subpath=用来标明该url是用url中某一部分自增来进行翻页
- 等号后面为正则表达式
    - 第一组用来匹配自增数字前面可能存在的部分（可为空）
    - 第二组用来匹配自增数字
    - 第三组来匹配自增数字后面可能存在的部分（可为空）
比如 http://www.timberland.com.hk/en/men-apparel-shirts，他的第二页变成了http://www.timberland.com.hk/en/men-apparel-shirts/page/2/，那么其中第一组就是/page/， 第二组是2, 第三组是/。
```
## item_xpath 配置方法
ITEM_XPATH ={ # 定义一组映射key(type=str):value(type=list)
    "amazon":[ # 你的爬虫的名字，一般可以是网站的名字
        '//div[@id="resultsCol"]//div[@class="a-row a-spacing-none"]/a[@class="a-link-normal a-text-normal"]/@href', # 链接xpath表达式，可以定义多个，所得到的结果为所有xpath结果的总和，和翻页xpath一样为了防止网站变来变去
        '//div[@id="mainResults"]/ul/li//a[@class="a-link-normal s-access-detail-page  a-text-normal"]/@href',
        '//div[@id="resultsCol"]//div[@class="a-row a-spacing-top-mini"]/a[@class="a-link-normal s-access-detail-page  a-text-normal"]/@href',
    ],
```
一般来说，网站每个项目的链接大都在服务端就将Html渲染完毕，几乎没有通过前端js渲染，我写过几百个网站，就遇到了一个前端js渲染生成项目链接，如果真遇到了这种情况，目前框架通用功能无能无力，只能在子类中重写parse函数。

## item_field 配置方法
```
ITEM_FIELD = {"bluefly": # spider名称
        [ # 要抓取的字段
            ('retail_price', { # 价格字段
                "xpath": [ # 提取方式, 还可以使用re, css表达式，按xpath, re, css顺序逐一提取，三者至少选择一个
                    '//*[@id="product-selection"]//span[@class="mz-price-retail-label"]/../text()', # 表达式
                    .... # 可以配置多个，如果第一个表达式提取为空，会使用下一个表达式提取
                ],
                "function": format_html_xpath_common, # 将提取结果转换成所需要的格式，可无，使用默认转换函数
                "extract": lambda item, response: image_urls_from(safely_json_loads(item["color_images"])), # 抽取函数，若转换函数调用完毕之后返回空，则会调用该函数，并返回结果，否则无效。 可无
                "default": "$100" # 默认值， 可无
                "request": lambda item, response: (urljoin(item['response_url'], xpath_exchange(
            response.xpath('//div[@id="availability"]/span/a/@href'))) if item['availability'] == 'other' else None, None, None, None), # 若通过function, extract之后得到的值仍然为空，则提供url做增发请求操作，用于在其它页面中获取所需值。其它页面抽取所需值的选择器及选择表达式可配置到xpath, re, css对应的表达式中，下一个页面使用的转换函数和抽取函数可以使用之前的函数，也可以单独配置，配置的名称为function_after以及extract_after，函数结构保持一致。可无
                "skip": True # 若为True，则这个字段仅存在于抓取函数处理过程中，pipline中会略过这个字段，可无
           }),
       ]
   }
```
通用函数的解释说明请参考[item_field函数配置](https://github.com/ShichaoMa/webWalker/wiki/item_field%E5%87%BD%E6%95%B0%E9%85%8D%E7%BD%AE)