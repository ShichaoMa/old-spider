框架是死的，人是活的，随着互联网的发展，我们要抓取的网站会变的越来越复杂，以至于现在的框架无法满足抓取的需求，难道我们就要束手无策了吗，不不不，还有更灵活的方式等着你去开发。
完成抓取任务的spider基类定义于walker/spider/__init__.py下，通过重写spider可以自定义抓取功能，什么该重写，这是一个好问题，以下是我的建议：
- 通过page_xpath配置获取下一页链接不好使的时候，重写parse函数
- 通过item_pxath配置获取项目链接不好使的时候，重写parse函数
其中用于抓取工作主要是parse和parse_item两个函数完成，下面将讲解其主要功能。
## parse函数
用于抓取分类链接中的所有item的url及所有下一页 url，并生成request放入任务队例中等待抓取
### parse函数主要功能：
  - 在meta中记录seed(分类链接)
  - 获取所有项目url并生成request
  - 在meta中记录该链接是否为翻页链接
  - 设置商品url及翻页url的回调函数
  - 设定商品url及翻页url的优先级
  - 获取翻页url并生成request

如要重写parse函数，请务必实现以上功能
demo 如下
```
@parse_method_wrapper
    def parse(self, response):

        self.logger.debug("start response in parse")
        # 获取item_url，请使用你的方式重写
        item_urls = [urljoin(response.url, x) for x in set(response.xpath("|".join(ITEM_XPATH[self.name])).extract())]
        # 从这一行开始，往下数13行，到###都需要带着
        self.crawler.stats.inc_total_pages(response.meta['crawlid'], len(item_urls))
        if "if_next_page" in response.meta:
            del response.meta["if_next_page"]
        else:
            response.meta["seed"] = response.url
        
        workers = response.meta.get('workers', {})

        for worker in workers.keys():
            workers[worker] = 0

        response.meta["callback"] = "parse_item"
        response.meta["priority"] -= 20
        #########################################
        # 这一部分是把所有url变成request,可以改
        for item_url in item_urls:
            response.meta["url"] = item_url
            yield Request(url=item_url,
                          callback=self.parse_item,
                          meta=response.meta,
                          errback=self.errback)
        # 这一部分是用来抓取下一页链接的，可以重写
        xpath = "|".join(PAGE_XPATH[self.name])

        if xpath.count("?") == 1:
            next_page_urls = [url_arg_increment(xpath, response.url)] if len(item_urls) else []
        elif xpath.count("subpath="):
            next_page_urls = [url_path_arg_increment(xpath, response.url)] if len(item_urls) else []
        elif xpath.count("/") > 1:
            next_page_urls = [urljoin(response.url, x) for x in set(response.xpath(xpath).extract())]
        else:
            next_page_urls = [url_item_arg_increment(xpath, response.url, len(item_urls))] if len(item_urls) else []
        # 下数三行都要带着
        response.meta["if_next_page"] = True
        response.meta["callback"] = "parse"
        response.meta["priority"] += 20
        # 这一部分是把所有一一页url变成request,可以改
        for next_page_url in next_page_urls:
            response.meta["url"] = next_page_url
            yield Request(url=next_page_url,
                          callback=self.parse,
                          meta=response.meta)
```

## parse_item函数 
用于抓取item的所有信息，包括标识号，标题，描述，图片，尺码，价格等信息，可以理解为所能获取到的全部信息
### parse_item函数主要功能介绍
- 1 构造 item，为item赋初始化值
- 2 为item增加配置的字段
- 3 为特定网站去重复item
- 4 记录抓取的数量
- 5 增发请求抓取特定字段

其实在绝大多数情况下，不需要重写parse_item函数，如果非要重写，请务必实现（1）（3）（4）功能，具体实现方式请参见父类设置

# 自定义下载中间件及pipline
## 自定义下载中间件
下载的过程中可能会遇到各种反爬虫机制，常规的反制措施框架都有提供，比如增加请求头user_agent，通过修改settings.py配置文件可以针对不同的spider配置不同的请求头如referer, cookies, host等等，同时也还提供了代理机制。重试，及重定向功能，但是，这些功能对于一些反爬能力比较强的网站是远远不够的，比如出验证码。这个时候，你需要自定义DownloadMiddle，通过继承walker.downloadermiddlewares.DownloaderBaseMiddleware, 自定义 process_reuqest, process_response, 或 process_exception，即可以轻松构造下载中间件，使用self.logger统计打印日志。具体方式可以查看源码中ProxyMiddleware的实现。
## 自定义pipline
对于数据集，我们最终可能会存储到不同的数据软件中，小到txt大到高性数据库，pipline提供了从字典类型数据（dict(item)）到其它格式数据的转换方式，通过继承walker.piplines.BasePipeline, 重写process_item，即可实现数据的多种模式存储。具体方式可以查看源码中JSONPipeline的实现。

